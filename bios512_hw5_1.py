# -*- coding: utf-8 -*-
"""BIOS512_hw5-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-u8h39cJMq_1oWgx-q7hl8DpgDq1zcT2

# Homework 5
This homework requires `wine.csv`, and the `tidyverse` and `Rtsne` packages. Install them if you haven't already!  
See the following link for how to add new packages to Binder: https://github.com/rjenki/BIOS512?tab=readme-ov-file#adding-packages-to-installr-later.   
**For readability and easier processing, please make each question part a different code chunk.**
"""

install.packages("Rtsne")

library(tidyverse)
library(Rtsne)

"""## Question 1  

#### a) Import your data.  
#### b) Check out the columns present using one of R's data frame summary.  
#### c) Get summary statistics on the numeric variables.  
"""

wine <- read_csv("wine.csv")
colnames(wine)
summary(wine)

"""## Question 2

#### a) Scale and center your data  
*Hint:* Use a `mutate()` statement across all columns **except class** with `function(x) as.numeric(scale(x))`.

#### b) Based on what you saw in the summary statistic table from the imported data, why would scaling and centering this data be helpful before we perform PCA?

"""

wine_scaled <- wine %>%
  mutate(across(-class, ~as.numeric(scale(.x))))

summary(wine_scaled)

"""Based on the summary statistics from the imported data, scaling is important before conducting PCA because some variables (like malicacid and total_phenols) are small in scale, while other variables like Proline are much much larger. Performing a PCA on these variables without scaling would make variables like Proline dominate the principal components but only because they are larger in scale and not importance. Scaling puts all variables on the same footing so the PCA reflects different patterns rather than measurments.

## Question 3

#### a) Perform PCA

#### b) How much of the total variance is explained by PC1? PC2? What function do we use to see that information?

#### c) Why are we doing PCA first?

#### d) What is the rotation matrix? Print it explicitly.  
*Hint:* Check the notes for a simple way to do this!

#### e) Plot PC1 vs. PC2, using the wine class as labels for coloring.  
*Hint:* You'll first need a data set with only PC1 and PC2, then add back the class variable from your scaled data set with a `mutate()` statement. Then, you can use `color = factor(class)` in your `ggplot` statement.

#### f) What do you see after plotting PC1 vs. PC2? What does this mean in context of wine classes?

#### g) Give an example of data where PCA would fail. You can describe the data or do a simulation.  
*Hint:* Our notes have a few examples!

#### h) Explain the difference between vector space and manifold, and how these terms apply to what we did/will do with T-SNE.
"""

# A) Perform PCA
wine_pca <- prcomp(wine_scaled %>% select(-class), center = FALSE, scale. = FALSE)
summary(wine_pca)

"""B) ~36% of the total variance is explained by PC1, and ~19% is explained by PC2. We use summary() function to see this information.

C) We do PCA first because it reduces 13 features into 2 or 3 components. Making it easier to see patterns or clusters in the wine data. It also prevents high-variance features (like Proline) from dominating.
"""

# D) The rotation matrix is loadings of each original feature on the PCs.
wine_pca$rotation

# E)
# Only PC1 and PC2
pc_df <- as.data.frame(wine_pca$x[, 1:2]) %>%
  rename(PC1 = PC1, PC2 = PC2)

# Add back the class variable
pc_df <- pc_df %>%
  mutate(class = wine_scaled$class)

# Plot PC1 vs PC2
ggplot(pc_df, aes(x = PC1, y = PC2, color = factor(class))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "PCA of Wine Data: PC1 vs PC2",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Wine Class")

"""F) There is clear seperation of wine classes into clusters (with few overlapping slightly). The three calsses are seperated along PC1 and PC2. This means that wines from the same class group together, so the chemical properties are good at distinguishing wine classes.

G) PCA would fail on data that has a non-linear structure. For example, data in a spiral shape.

H) A vector space is a flat, Euclidean space where PCA works (has linear relationships, orthogonal directions). A manifold is a curved, nonlinear space that is embedded in higher dimensions. Data can "live" on lower-dimensional surface that isn't flat. PCA captures linear variance, while T-SNE captures nonlinear structures, so it can reveal cluters when PCA fails.

## Question 4
#### a) Perform T-SNE
Set `seed = 123`.  
*Hint:* Subset your PCA results to PC1â€“PC10, add the class variable back in, remove duplicates, then perform T-SNE.

#### b) Plot the results in 2D
*Hint:* Convert your T-SNE results to a tibble and add back the class variable from your scaled data set using a `mutate()` statement. Then, you can use `color = factor(class)` in your `ggplot` statement.

#### c) Why didn't we stop at PCA?


#### d) What other types of data does this workflow make sense for?
"""

set.seed(123)

# Take first 10 PCs
pca10 <- as.data.frame(wine_pca$x[, 1:10]) %>%
  mutate(class = wine_scaled$class) %>%
  distinct()   # remove duplicates

# Run T-SNE
wine_tsne <- Rtsne(pca10 %>% select(-class), dims = 2, perplexity = 30, verbose = TRUE)

# Convert to tibble
tsne_df <- as_tibble(wine_tsne$Y) %>%
  rename(Dim1 = V1, Dim2 = V2) %>%
  mutate(class = pca10$class)

# Plot
ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = factor(class))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "t-SNE on Wine Data",
       x = "t-SNE Dimension 1",
       y = "t-SNE Dimension 2",
       color = "Wine Class")

"""C) We didn't stop at PCA because PCA only captures linear variance. Some data structures are nonlinear manifolds so PCA will miss those patterns. T-SNE also preserves local neighborhood relationships so it makes clusters of similar points clearer. So we already showed seperation with PCA, but T-SNE makes the class clusters even more distinct.

D) This workflow would make sense for high-dimensional data (like more than 50 features) or data that may lie on a nonlinear manifold. For example, images (pixels are high-dimensional).
"""